# Running ollama third party Service
## Choosing a Model
You can get the model_id that ollama will launch from using the [Ollama Library](https://ollama.com/library) 
For example, https://ollama.com/library/llama3.2:1b

Q - For the LLM service, which can do text generation, it suggests it will only work with TGI or vLLM and all you have to do is have it running. Does TGI/vLLM have a standardized api or is there code to dectect which one is running? Do we really have to use xeon or gaudi processor?